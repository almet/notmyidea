<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alexis Métaireau - llm</title><link href="https://blog.notmyidea.org/" rel="alternate"></link><link href="https://blog.notmyidea.org/feeds/tags/llm.atom.xml" rel="self"></link><id>https://blog.notmyidea.org/</id><updated>2023-09-27T00:00:00+02:00</updated><entry><title>llm command-line tips</title><link href="https://blog.notmyidea.org/llm-command-line-tips.html" rel="alternate"></link><published>2023-09-27T00:00:00+02:00</published><updated>2023-09-27T00:00:00+02:00</updated><author><name></name></author><id>tag:blog.notmyidea.org,2023-09-27:/llm-command-line-tips.html</id><summary type="html">&lt;p&gt;I&amp;#8217;m using &lt;a href="https://llm.datasette.io"&gt;llm&lt;/a&gt; more and more, and today I had to find back prompts I used in the past. Here is a command I&amp;#8217;ve been using, which allows me to filter the results based on what I want. It leverages &lt;a href="https://sqlutils.datasette.io"&gt;sql-utils&lt;/a&gt;, a cli tool which is able to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I&amp;#8217;m using &lt;a href="https://llm.datasette.io"&gt;llm&lt;/a&gt; more and more, and today I had to find back prompts I used in the past. Here is a command I&amp;#8217;ve been using, which allows me to filter the results based on what I want. It leverages &lt;a href="https://sqlutils.datasette.io"&gt;sql-utils&lt;/a&gt;, a cli tool which is able to talk to a &lt;span class="caps"&gt;SQLITE&lt;/span&gt; database and answer in json, and &lt;a href="https://github.com/jqlang/jq"&gt;jq&lt;/a&gt; a command-line tool capable of doing requests for&amp;nbsp;json.&lt;/p&gt;
&lt;p&gt;All in all, it&amp;#8217;s pretty satisfying to use. I finally got a simple way to query databases! I&amp;#8217;m also using &lt;a href="https://github.com/charmbracelet/glow"&gt;glow&lt;/a&gt;, which is capable of transforming markdown into a better version on the&amp;nbsp;terminal.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sqlite-utils&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;llm&lt;span class="w"&gt; &lt;/span&gt;logs&lt;span class="w"&gt; &lt;/span&gt;path&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;SELECT * FROM responses WHERE prompt LIKE &amp;#39;%search%&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;jq&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.[].response&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-r&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;glow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Which got me a colored response&amp;nbsp;:-)&lt;/p&gt;</content><category term="code"></category><category term="python"></category><category term="llm"></category><category term="bash"></category><category term="sqlite"></category></entry><entry><title>How to run the vigogne model locally</title><link href="https://blog.notmyidea.org/how-to-run-the-vigogne-model-locally.html" rel="alternate"></link><published>2023-09-22T00:00:00+02:00</published><updated>2023-09-22T00:00:00+02:00</updated><author><name></name></author><id>tag:blog.notmyidea.org,2023-09-22:/how-to-run-the-vigogne-model-locally.html</id><summary type="html">
&lt;p&gt;&lt;a href="https://github.com/bofenghuang/vigogne"&gt;Vigogne&lt;/a&gt; is a &lt;span class="caps"&gt;LLM&lt;/span&gt; model based on &lt;span class="caps"&gt;LLAMA2&lt;/span&gt;, but trained with french data. As I&amp;#8217;m working mostly in french, it might be useful. The current models that I can get locally are in&amp;nbsp;english.&lt;/p&gt;
&lt;p&gt;The information I&amp;#8217;ve found online are scarse and not so easy to follow, so …&lt;/p&gt;</summary><content type="html">
&lt;p&gt;&lt;a href="https://github.com/bofenghuang/vigogne"&gt;Vigogne&lt;/a&gt; is a &lt;span class="caps"&gt;LLM&lt;/span&gt; model based on &lt;span class="caps"&gt;LLAMA2&lt;/span&gt;, but trained with french data. As I&amp;#8217;m working mostly in french, it might be useful. The current models that I can get locally are in&amp;nbsp;english.&lt;/p&gt;
&lt;p&gt;The information I&amp;#8217;ve found online are scarse and not so easy to follow, so here is a step by step tutorial you can follow. I&amp;#8217;m using &lt;a href="https://pipenv.pypa.io/en/latest/"&gt;pipenv&lt;/a&gt; almost everywhere now, it&amp;#8217;s so easy&amp;nbsp;:-)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;llm&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;-U&lt;span class="w"&gt; &lt;/span&gt;llm-llama-cpp
wget&lt;span class="w"&gt; &lt;/span&gt;https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q4_K_M.gguf
llm&lt;span class="w"&gt; &lt;/span&gt;llama-cpp&lt;span class="w"&gt; &lt;/span&gt;add-model&lt;span class="w"&gt; &lt;/span&gt;vigogne-2-7b-chat.Q4_K_M.gguf&lt;span class="w"&gt; &lt;/span&gt;-a&lt;span class="w"&gt; &lt;/span&gt;vigogne
llm&lt;span class="w"&gt; &lt;/span&gt;models&lt;span class="w"&gt; &lt;/span&gt;default&lt;span class="w"&gt; &lt;/span&gt;vigogne
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="code"></category><category term="llm"></category></entry></feed>