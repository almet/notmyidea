<!DOCTYPE html>
<html lang="en">

<head>
    <title>How to run the vigogne model&nbsp;locally - Alexis Métaireau</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://blog.notmyidea.org/theme/css/main.css" type="text/css" />
    <link href="https://blog.notmyidea.org/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
        title="Alexis Métaireau ATOM Feed" />
</head>

<body>
    <section id="links">
    <ul>
        <li><a class="main" href="/">Alexis Métaireau</a></li>
        <li><a class=""
            href="https://blog.notmyidea.org/journal/index.html">Journal</a></li>
        <li><a class="selected"
            href="https://blog.notmyidea.org/code/">Code, etc.</a></li>
        <li><a class=""
            href="https://blog.notmyidea.org/weeknotes/">Notes hebdo</a></li>
        <li><a class=""
            href="https://blog.notmyidea.org/lectures/">Lectures</a></li>
    </ul>
    </section>

<header>
	<h1 class="post-title">How to run the vigogne model&nbsp;locally</h1>
	<time datetime="2023-09-22T00:00:00+02:00">22 septembre 2023</time>

    
</header>
<article>
    
<p><a href="https://github.com/bofenghuang/vigogne">Vigogne</a> is a <span class="caps">LLM</span> model based on <span class="caps">LLAMA2</span>, but trained with french data. As I&#8217;m working mostly in french, it might be useful. The current models that I can get locally are in&nbsp;english.</p>
<p>The information I&#8217;ve found online are scarse and not so easy to follow, so here is a step by step tutorial you can follow. I&#8217;m using <a href="https://pipenv.pypa.io/en/latest/">pipenv</a> almost everywhere now, it&#8217;s so easy&nbsp;:-)</p>
<div class="highlight"><pre><span></span><code>llm<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>llm-llama-cpp
wget<span class="w"> </span>https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q4_K_M.gguf
llm<span class="w"> </span>llama-cpp<span class="w"> </span>add-model<span class="w"> </span>vigogne-2-7b-chat.Q4_K_M.gguf<span class="w"> </span>-a<span class="w"> </span>vigogne
llm<span class="w"> </span>models<span class="w"> </span>default<span class="w"> </span>vigogne
</code></pre></div>
	<p>
        <a href="https://blog.notmyidea.org/tag/llm.html">#llm</a>	  - Posté dans la catégorie <a href="https://blog.notmyidea.org/code/">code</a>
	</p>
</article>
    <footer>
    <a id="feed" href="/feeds/all.atom.xml"><img src="/theme/rss.svg" /></a>
    </footer>

</body>

</html>